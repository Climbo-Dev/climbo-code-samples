{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd8332f0-8c8a-4a28-aff4-c3c3fd21237a",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory (LSTM)\n",
    "\n",
    "*This notebook is created with assistance from [ChatGPT 4](https://openai.com/gpt-4).*\n",
    "\n",
    "A good article about the LSTM model is [*Understanding LSTM Network*](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Christopher Olah, a former OpenAI researcher.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Climbo-Dev/climbo-code-samples/main/images/LSTM3-chain.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8f23f739-0aba-4dfe-a8da-f46d6ad15b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code here is generated by ChatGPT 4\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Weights for x\n",
    "        self.Wi = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
    "        self.Wf = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
    "        self.Wg = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
    "        self.Wo = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
    "\n",
    "        # Weights for h\n",
    "        self.Ri = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.Rf = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.Rg = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.Ro = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.data.ndimension() >= 2:\n",
    "                nn.init.xavier_uniform_(p.data)\n",
    "            else:\n",
    "                nn.init.zeros_(p.data)\n",
    "\n",
    "    def forward(self, x, init_states):\n",
    "        batch_size = x.size(0)\n",
    "        h_t, c_t = init_states\n",
    "\n",
    "        i_t = torch.sigmoid(x @ self.Wi + h_t @ self.Ri)\n",
    "        f_t = torch.sigmoid(x @ self.Wf + h_t @ self.Rf)\n",
    "        g_t = torch.tanh(x @ self.Wg + h_t @ self.Rg)\n",
    "        o_t = torch.sigmoid(x @ self.Wo + h_t @ self.Ro)\n",
    "        \n",
    "        c_next = f_t * c_t + i_t * g_t\n",
    "        h_next = o_t * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm_cell = LSTMCell(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        sequence_length = x.size(1)\n",
    "\n",
    "        h_t = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "        c_t = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "\n",
    "        for i in range(sequence_length):\n",
    "            h_t, c_t = self.lstm_cell(x[:, i, :], (h_t, c_t))\n",
    "\n",
    "        out = self.fc(h_t)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d261851-ec9a-46b0-978c-2d2044ca38e9",
   "metadata": {},
   "source": [
    "ChatGPT 4 suggested a few datasets that could be used for training an LSTM model:\n",
    "\n",
    "- **IMDB Movie Reviews Dataset**: This dataset contains 50,000 movie reviews that are marked as either positive or negative. You can train an LSTM model for sentiment analysis on this dataset. This dataset is available in the keras.datasets module in Python.\n",
    "\n",
    "- **Human Activity Recognition Using Smartphones Data Set**: This dataset contains data from the accelerometer and gyroscope of a smartphone, capturing human activities like WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING. You can find this dataset on the UCI Machine Learning Repository. An LSTM model could be used to recognize these activities based on the sensor data.\n",
    "\n",
    "- **Air Passengers Data Set**: This dataset contains monthly totals of international airline passengers from 1949 to 1960. You can use an LSTM model to forecast the number of airline passengers. This dataset is available in the datasets module of the seaborn library in Python (seaborn.load_dataset(\"flights\")).\n",
    "\n",
    "- **Penn Tree Bank (PTB) Dataset**: This dataset is a large collection of sentences, widely used for training and benchmarking models for natural language processing tasks, such as language modeling. LSTMs are often used for such tasks because of their ability to capture long-term dependencies in the data.\n",
    "\n",
    "- **Daily Minimum Temperatures in Melbourne**: This dataset describes the minimum daily temperatures over 10 years (1981-1990) in Melbourne, Australia. The units are in degrees Celsius and there are 3,650 observations. This dataset can be used for univariate time series forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4c554a-1053-45f2-ac58-ab5c15fcb965",
   "metadata": {},
   "source": [
    "## IMDB Movie Reviews - Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9fda2a-3e1f-42d0-8aa8-a4e7cf7a3279",
   "metadata": {},
   "source": [
    "A good reference is [Getting started with NLP for absolute beginners](https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners).\n",
    "\n",
    "This notebook focuses on a minimum example on model training. The Exploratary Data Analysis (EDA) can be found in a sibling notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359c6698-8349-4d91-a241-3cbf3d7a13ba",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce690081-8a00-4628-943d-6cb87c6f47f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fastai.data.external import fastai_cfg, URLs, untar_data\n",
    "\n",
    "imdb_path = untar_data(URLs.IMDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ac4786-1372-4360-bc3c-f2d4c750f841",
   "metadata": {},
   "source": [
    "### Using Native Python + PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2366b083-572e-4ab5-bba2-fe117605b1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "from torch.nn.functional import binary_cross_entropy\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokz = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f78d10-39b6-4ee4-81b0-e08e20cafc05",
   "metadata": {},
   "source": [
    "#### Train a LSTM Model with Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "870ef9fb-75e8-40b6-829f-bdb661a9c32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.DataFrame([\n",
    "    dict(sentiment=1, text=\"This movie is one of the best\"),\n",
    "    dict(sentiment=0, text=\"This movie is among the worst\"),\n",
    "])\n",
    "sample[\"tokens\"] = sample.text.apply(lambda t: tokz(t, return_tensors='pt').input_ids.squeeze())\n",
    "\n",
    "# Model Configuration\n",
    "embedding_dim = 10\n",
    "learning_rate = 0.01\n",
    "args = pd.Series(dict(input_size=embedding_dim, hidden_size=5, output_size=1))\n",
    "\n",
    "# Setup Model & Toolkit\n",
    "embed = torch.nn.Embedding(len(tokz), embedding_dim)\n",
    "\n",
    "model = LSTM(**args)\n",
    "\n",
    "preds, targets = [], []\n",
    "for _, r in sample.iterrows():\n",
    "    x_embed = embed(r.tokens)[None,:]\n",
    "    p = model(x_embed)\n",
    "    preds.append(p)\n",
    "    targets.append(r.sentiment)\n",
    "\n",
    "probs = torch.sigmoid(torch.concat(preds).squeeze())\n",
    "targets = torch.tensor(targets).to(torch.float32)\n",
    "loss = binary_cross_entropy(probs, targets)\n",
    "loss.backward()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for param in model.parameters():\n",
    "        param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5824aca6-d817-4061-9f57-77156954b6ab",
   "metadata": {},
   "source": [
    "#### Train a full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cc4de687-bd70-433c-89f1-6952a2a5cad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "from glob import glob\n",
    "from itertools import product\n",
    "\n",
    "sentiments, tokens_list = [], []\n",
    "\n",
    "for s, folder in [(1, \"pos\"), (0, \"neg\")]:\n",
    "    for f in (imdb_path / 'train' / folder).glob('*.txt'):\n",
    "        with open(f, 'r') as f:\n",
    "            text = f.read().strip()\n",
    "            sentiments.append(s)\n",
    "            tokens_list.append(tokz(text, return_tensors='pt').input_ids.squeeze())\n",
    "\n",
    "df = pd.DataFrame(dict(sentiment = np.float_(sentiments), tokens = tokens_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "091b690d-03de-4bf1-b0ff-cb4f8484a655",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pct = 0.2\n",
    "num_valid = int(df.shape[0] * valid_pct)\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "valid = df.iloc[:num_valid, :].reset_index(drop=True)\n",
    "train = df.iloc[num_valid:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d9431a0e-a98c-4bb4-91f2-b519cb8c20e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "bar_format='{l_bar}{bar:100}{r_bar}{bar:-100b}'\n",
    "\n",
    "embedding_dim = 10\n",
    "args = pd.Series(dict(input_size=embedding_dim, hidden_size=5, output_size=1))\n",
    "\n",
    "# Setup Model & Toolkit\n",
    "embed = torch.nn.Embedding(len(tokz), embedding_dim, dtype=torch.float16)\n",
    "model = LSTM(**args).half()\n",
    "\n",
    "def evaluate(data):\n",
    "    preds, targets = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, r in tqdm(list(data.iterrows()), bar_format=bar_format):\n",
    "            x_embed = embed(r.tokens)[None,:]\n",
    "            p = model(x_embed)\n",
    "            preds.append(p)\n",
    "            targets.append(r.sentiment)\n",
    "    \n",
    "    probs = torch.sigmoid(torch.concat(preds).squeeze())\n",
    "    targets = torch.tensor(targets).to(torch.float32)\n",
    "    loss = binary_cross_entropy(probs, targets)\n",
    "    return np.float_(loss), pd.DataFrame(dict(prob=probs.detach().numpy(), target=targets.detach().numpy()))\n",
    "# evaluate(valid[:10])\n",
    "\n",
    "def one_epoch(data, bs, learning_rate: float=0.01):\n",
    "    num_train = data.shape[0]\n",
    "    \n",
    "    df = data.sample(frac=1).reset_index(drop=True)\n",
    "    df['batch'] = np.repeat(range(num_train // bs + 1), bs)[:num_train]\n",
    "    \n",
    "    for _, sdf in tqdm(list(df.groupby(\"batch\")), bar_format=bar_format):\n",
    "        preds, targets = [], []\n",
    "        for _, r in sdf.iterrows():\n",
    "            x_embed = embed(r.tokens)[None,:]\n",
    "            p = model(x_embed)\n",
    "            preds.append(p)\n",
    "            targets.append(r.sentiment)\n",
    "        \n",
    "        probs = torch.sigmoid(torch.concat(preds).squeeze())\n",
    "        targets = torch.tensor(targets)\n",
    "        loss = binary_cross_entropy(probs, targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e8263b3-f583-4de1-859d-62631adc3d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [01:45<00:00, 47.60it/s]                                                                                                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss:   0.704275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "valid_loss, _ = evaluate(valid)\n",
    "print(f\"valid loss: {valid_loss:10.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6264abd3-7652-4175-bbe6-c4ec72b84797",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda\")\n",
    "for _ in range(5):\n",
    "    one_epoch(train, bs=32, learning_rate=0.01)\n",
    "    valid_loss, _ = evaluate(valid)\n",
    "    print(f\"valid loss: {valid_loss:10.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf8798d-ef31-4964-8749-d878a37b92c7",
   "metadata": {},
   "source": [
    "\n",
    "### Using Fast AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "0db122a7-4075-4501-9e5e-9d30041a4a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.data import TextDataLoaders"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
